# スマートに学習する

ニューラルネットワークの学習について、精度を上げるために様々な方法が研究されている。  

## 学習とテストは別々のデータで

ニューラルネットワークの学習に用いるデータを **訓練(教師)データ**　という。  
学習が完了したとき、ニューラルネットワークは訓練データに対してはそれなりの精度で推測ができる(はずだ)が、未知のデータでどれだけ精度の高い推測ができるかが重要である。  
そのため、訓練データとは別の **テストデータ** を使って、ニューラルネットワークの能力を評価する。  

## 各層のパーセプトロンの出力値をばらけさせる

ニューラルネットワークではパーセプトロンが多層構造になってるが、それぞれの層において、パーセプトロンの出力値が適度に分布していることが重要になる。  
同じ値を出力するパーセプトロンが同じ層に多数存在している場合は、それらを1つのパーセプトロンに置き換えても同じことになるため意味がなくなってしまう。  
パーセプトロンの出力値を分布させるために、以下の方法がある。  

 * 適切な重みの初期化  
重みの初期値には、使用する活性化関数によって適した初期化の方法があることがわかっている。  
シグモイド関数には **Xavierの初期値** を、ReLU関数には **Heの初期値**　を用いる。  
※いずれも「特定の値」を標準偏差とした正規分布から、ランダムで各重みに初期値をする。  

* **Batch Normalization**  
パーセプトロン内にて、活性化関数を適用する前に値を適度な分布(平均0、分散1の分布)に変換する方法。

## 本番に弱い「過学習」

訓練データでは良好な精度を出せるが、テストデータでは精度が出ない状態を **過学習** という。  
(訓練データ内の「癖(本質的ではない特徴)」に特化して反応してしまうようなイメージ。)  
過学習を抑止する方法として、以下のものがある。

* **荷重減衰(Waight decay)**  
過学習は重みが大きな値となることで発生することがしばしばある。  
そのため、重みが大きくならないように調整するのが荷重減衰である。  
具体的には誤差逆伝播法で求めた勾配に対して
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;\lambda W"/>
を加える。  
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;\lambda"/>
は荷重減衰の強さを決定するパラメータであり、
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;W"/>
は重みを表している。  
重みが大きいほど勾配が大きくなる。学習では勾配に学習率を乗算し、重みから引くので、勾配が大きいほど重みは小さくなるという仕掛けになっている。  
※重みの2乗和を使うL2ノルムの場合。他にも重みの絶対値の総和であるL1ノルムなどがある。

* **Dropout**  
パーセプトロンをランダムに消去しながら学習する方法。  
疑似的な **アンサンブル学習** となっている。  
アンサンブル学習とは、複数の同じ(あるいは類似の)ニューラルネットワークをそれぞれに学習させて、推論ではこれら複数のニューラルネットワークの推論値を平均する手法。  
過学習が訓練データ内の「癖」に着目してしまうなら、複数のニューラルネットワークで推論をして「癖」への着目の影響を平均化して抑え込むようなイメージ。  

## 人が設定するパラメータにも注意を

ニューラルネットワークの学習により、重みとバイアスは適切な値に調整されるが、それ以外のものについては人力で適したものを探していく必要がある。  
そういったパラメータを **ハイパーパラメータ** といい、ニューラルネットワークの階層数、層内のパーセプトロンの数、バッチサイズ、学習率、荷重減衰の強さなどがある。  
ハイパーパラメータの調整には、学習データともテストデータとも異なる **検証データ** を用意する必要がある。  
学習データを使ってハイパーパラメータを調整すると、学習データに対してのみ適合してしまう可能性があるからである。  
