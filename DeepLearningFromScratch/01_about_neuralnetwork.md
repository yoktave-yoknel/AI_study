# ニューラルネットワークについて

## ニューラルネットワークは大きな関数

ニューラルネットワークとは、複数のパーセプトロンを多層に連結することで構成されている。  
パーセプトロンは以下の関数として表すことができる。  
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;y=h(\sum&space;w_nx_n&plus;b)"/>  
ここに出てくる記号は以下のものを表している。  
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;x_n"/>: (複数の)入力値  
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;w_n"/>: それぞれの入力値に乗算する **重み** (weight)  
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;b"/>: 重みを掛けた入力値を合計し、そこに加算する **バイアス** (bias)  
<img src="https://latex.codecogs.com/gif.latex?\bg_white&space;h()"/>: **活性化関数** (上記の計算結果を0～1の値に変換するもの)  

パーセプトロンが関数であるということは、これを組み合わせたニューラルネットワークも関数(パーセプトロンの合成関数)である。  
活性化関数には **シグモイド関数** などが使用される。(ニューラルネットワークの最後では別の関数を使用する。後述参照。)  


## 回帰/分類で活性化関数を使い分け

ニューラルネットワークとは、入力値を与えると「何らかの推論値」を出力する関数であるとみなすことができる。  
この推論値については、ニューラルネットワークで解こうとしているものが **回帰問題** であるか **分類問題** であるかによって少し扱いが変わってくる。  

回帰問題とは連続的な数値の予測を行うもので、例えば株価の予想など。  
分類問題はどのクラスに属するかを予測するもので、例えば画像が犬であるか猫であるかを予測するといった具合である。  

分類問題は、クラスAに属する可能性が〇%、クラスBに属する可能性が×%...という形で表現されるため、ニューラルネットワークが出力する推論値は確率の形式をとる必要がある。  
確率への変換を行うために **ソフトマックス関数** が用いられる。  
そのため、分類問題ではニューラルネットワークの最後の活性化関数としてソフトマックス関数を使用する。  
一方、回帰問題ではニューラルネットワークの推論値を、そのまま最終的な出力値として使用することができる。  
引数をそのまま返す関数を **恒等関数** というので、回帰問題ではニューラルネットワークの最後の活性化関数は恒等関数となる。

## ニューラルネットワークの学習とは重みとバイアスを調整すること

ニューラルネットワークが出力する推論値がどれほど正確であるかは、ニューラルネットワークの中で使用する重みとバイアスに依存している。  
推論値がより正確になるように、重みとバイアスを調整することを **学習** という。  
学習を行うには、 以下の2つのことが必要となる。  

* ニューラルネットワークが出力した推論値と正解との誤差を算出  
誤差は、推論値と正解を **損失関数** に入力することで求められる。  
損失関数には **2乗和誤差** や **交差エントロピー** がある。  
どの損失関数を使用するかは、分類問題であるか回帰問題であるかによって決まってくる。(正確には、最後に使用する活性化関数に対応)  

|問題の種類|最後に使用する活性化関数|誤差関数|
|:---|:---|:---|
|分類問題|ソフトマックス関数|交差エントロピー|
|回帰問題|恒等関数|2乗和誤差|

* 誤差を減少させるように重みとバイアスを更新  
ここで問題となるのは「誤差を減少させるには、それぞれの重みとバイアスを増減どちらの方向にどれだけ調整すればいいか」ということである。  
端的に言うと、損失関数を微分することでそれがわかるようになる。  
微分とは、関数のパラメータを現在値から微少量動かした際に、関数の出力値がどのように変化するかを算出したものである。  
例えば、ある重みの変化が損失関数の出力値に対してプラスの方向に動くなら、その重みをマイナスの方向に更新することで、損失関数の出力値が減少することが期待できる。

上記の2つのことを繰り返して、損失関数の出力値が小さくなるような重みとバイアスの値を探していくことが、ニューラルネットワークにおける学習となる。
