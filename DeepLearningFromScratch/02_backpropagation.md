# 誤差逆伝播法と微分と勾配降下の話

## 誤差逆伝播法とは微分のアルゴリズム

ニューラルネットワークでの学習について **誤差逆伝播法(Backpropagation)** が用いられている。   
ニューラルネットワークの学習とは、重みとバイアスの調整を行い、誤差関数の出力値を小さくしていくことである。  
ここで、ニューラルネットワークと誤差関数は、まとめて1つの巨大な関数とみなすことができる。また、重みとバイアスはこの関数のパラメータとなっている。  
パラメータを微少量変化させた際に、関数の出力値がどのように変動するかを求めるのが **微分** である。  
ただし、この巨大な関数をいきなり微分することは難しい。  
そこで **巨大な関数を構成する一つ一つの関数に対して、それぞれに微分** を行うことで全体の微分を行うというアルゴリズムを用いる。  
このアルゴリズムでは、出力に近い方の関数から順番に微分を行っていく。入力から出力に流れる方向(順伝播)に対して反対の方向に処理をしていくので **逆伝播** という。  
こうして、誤差関数の出力値(これがニューラルネットワークの **誤差** となっている)に対する重みやバイアスの微分を行っていくのが誤差逆伝播法である。  

## グラフの傾きを下って誤差の最小点を探す

関数のグラフを平面上に描いた際に、微分の値は **傾き** として現れる。  
これは関数のパラメータが多次元になったときも同様であり、その時の傾き(多次元のベクトルとして表現される)を **勾配** という。  
ニューラルネットワークの学習では、誤差を小さくすることが目的となる。  
これはグラフで考えたときに、勾配を下っていく方向にパラメータを調整することである。  
こうして、勾配を下る方向にパラメータを調整することを **勾配降下法** という。  
また、勾配の方向にどれだけ進むかを **学習率** という。  
学習率は手動で適当な値を設定する必要がある。  
大きすぎれば目的地を行き過ぎてしまうし、小さすぎれば目的値に到達するのに時間がかかる。  
このように、勾配の降下をどのように行うかで、ニューラルネットワークの学習の効率に大きな違いが生じる。  
主な勾配降下法としては以下のものがある。  

|方法|memo|
|:---|:---|
|確率的勾配降下法(SGD)|シンプルな方法だが非効率なこともある|
|Momentum|降下に運動量の概念を取り入れ「行き過ぎ」を抑止|
|AdaGrad|学習率を逓減して「行き過ぎ」を抑止|
|Adam|MomentumとAdaGradの折衷|


## 微分の値が0になると困ることに...

ニューラルネットワークの学習においては **微分の値が0でない** ということが重要な意味を持っている。  
もし、微分の値が0になる、つまり勾配が水平になってしまうと、パラメータを増減どちらの方向に移動しても、誤差を小さくすることができず、学習が進まなくなってしまう。  
この状態を **勾配消失** という。  
勾配消失を回避する一つの方法として、活性化関数に **ReLU(Rectified Linear Unit)関数** を用いることが挙げられる。  
