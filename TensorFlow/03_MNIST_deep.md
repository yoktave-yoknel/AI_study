# チュートリアル_手書き文字(その2)

TensorFlow公式サイトのチュートリアルでは、次に畳み込みニューラルネットワークを使って手書き文字認識を行う例が提示されている。  
Deep MNIST for Experts  
https://www.tensorflow.org/get_started/mnist/pros

こちらにも日本語解説サイトがあるので併せて見ていく。  
http://tensorflow.classcat.com/2016/03/10/tensorflow-cc-deep-mnist-for-experts/

処理の内容をコメントしたソースコードはこちら  
[mnist_deep.py](../source/TF_MNIST/mnist_deep.py)  
→実行はかなり時間がかかる。一晩置いておくくらいの感覚。  
　(PCスペックはその1にて記載)  
認識精度は99%になった。  
~~~
test accuracy 0.9922
~~~
## 内容について

前回のチュートリアルではすべてのピクセルを1列に並べて計算をしたが、今回は画像を平面として扱う。  
(位置のずれの影響を抑えるため)  

畳み込みニューラルネットワークでは以下の処理を行う。  
<第1層畳み込み>  
　手書き文字画像に対し、5\*5のサイズのフィルターを適用。  
　フィルターを1画素ずつ移動して重み付けを行う。  
　1つの手書き文字画像から、32個のフィルター適用済みデータ(特徴マップ)を作成する。  
　活性化関数にはReLU(Rectified Linear Unit)を用いる。  
　※畳み込み層ではReLUを使用することが多い。  
<第1層プーリング>  
　特徴マップのサイズを削減する。  
　2\*2の最大プーリングを実施することで、28\*28の画像が半分の14\*14になる。  
<第2層畳み込み>  
　第1層プーリングの出力である、サイズ14\*14、32個の特徴マップに対し、  
　第1層畳み込みと同様に5\*5のサイズのフィルターを適用し、64個の特徴マップに拡張する。  
　活性化関数も第1層畳み込みと同じくReLUを使用する。  
<第2層プーリング>  
　第1層プーリングと同様。14\*14から7\*7に削減する。  
<全結合層>  
　画像全体の処理を可能にするために、1024ノードの全結合層を追加する。  
　第2層プーリングの出力(サイズ7\*7、特徴マップ64個)を、前回のチュートリアルのように1列に並べる。  
　1024ノードの全結合層へ重み付けとバイアス加算を行い、活性化関数のReLUを適用する。  
<ドロップアウト>  
　過学習を避けるためにドロップアウトを適用する。  
　このチュートリアルでは、学習時には50%のノードのみ使用するようにする。  
　つまり、全結合層の1024ノードのうち、学習では半分の512ノードを使用して過学習を避け、  
　テストデータによる検証時には1024ノードすべてで検証するようにする。  
<読み出し層>  
　前回のチュートリアルと同様に、全結合層の1024ノードから10種類の数字の確率を算出する。  

## 参考

畳み込みニューラルネットワークについて  
https://qiita.com/nvtomo1029/items/601af18f82d8ffab551e  
https://deepage.net/deep_learning/2016/11/07/convolutional_neural_network.html  

TensorFlowの関数について  
https://qiita.com/tadOne/items/b484ce9f973a9f80036e
